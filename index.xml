<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Łukasz Kidziński on Łukasz Kidziński</title>
    <link>/</link>
    <description>Recent content in Łukasz Kidziński on Łukasz Kidziński</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Łukasz Kidziński</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Recent &amp; Upcoming Talks</title>
      <link>/not-used/talks/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/not-used/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent Posts</title>
      <link>/not-used/posts/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/not-used/posts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Street view accident risk</title>
      <link>/project/street-view/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/street-view/</guid>
      <description>&lt;p&gt;Road traffic injuries are a leading cause of death worldwide. Proper estimation of car accident risk is critical for appropriate allocation of resources in healthcare, insurance, civil engineering, and other industries. We show how images of houses are predictive of car accidents. We analyze 20,000 addresses of insurance company clients, collect a corresponding house image using Google Street View, and annotate house features such as age, type, and condition. We find that this information substantially improves car accident risk prediction compared to the state-of-the-art risk model of the insurance company and could be used for price discrimination. From this perspective, public availability of house images raises legal and social concerns, as they can be a proxy of ethnicity, religion and other sensitive data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.05270&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/1904.05270&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.technologyreview.com/s/613432/how-a-google-street-view-image-of-your-house-predicts-your-risk-of-a-car-accident/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;../../img/logos/mit1.png&#34; alt=&#34;MIT Technology Review&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Google Street View image of a house predicts car accident risk of its resident</title>
      <link>/publication/street-view/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/street-view/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal data analysis using matrix completion</title>
      <link>/publication/fcomplete/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/fcomplete/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>fcomplete</title>
      <link>/project/longitudinal-data/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/longitudinal-data/</guid>
      <description>&lt;p&gt;Suppose we observe N subjects, each subject at multiple timepoints and we want to estimate a trajectory of progression of measurements in individual subjects. For example, suppose you observe BMI of N children at different ages, as presented below&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/kidzinski/kidzinski/fcomplete/grouped.png&#34; width=450 /&gt;
&lt;/p&gt;

&lt;p&gt;Here, the connected dots come from individual subjects and the black thick line corresponds to the population mean.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I&amp;rsquo;ve build the package to fit trajectories using matrix completion and I described the methodology in my recent paper &lt;a href=&#34;https://arxiv.org/abs/1809.08771&#34; target=&#34;_blank&#34;&gt;Kidziński, Hastie (2018)&lt;/a&gt;.&lt;/strong&gt; To this end, we discretize the time grid some continous basis and find a low-rank decomposition of the dense matrix.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/kidzinski/kidzinski/fcomplete/intro-1.png&#34; alt=&#34;Matrix completion and sparse longitudinal completion&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the classical matrix completion, we look for matrices &lt;code&gt;W&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt; that fit the observed points in &lt;code&gt;Y&lt;/code&gt; (green points in the image above). In our method, in order to impose smoothness, we additionaly assume the basis &lt;code&gt;B&lt;/code&gt; and again we look for the reprezentation minimizing the errror.&lt;/p&gt;

&lt;p&gt;The interface of the package is based on the mixed-effect models in &lt;code&gt;R&lt;/code&gt;. In particular, if we are given temporal observations &lt;code&gt;Y&lt;/code&gt; in the long format with columns &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;bmi&lt;/code&gt;, while additional covariates &lt;code&gt;X&lt;/code&gt;, constant over time are given as a data frame with columns &lt;code&gt;id&lt;/code&gt; and, say, &lt;code&gt;gender&lt;/code&gt;, we can fit the model by writing&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;model = fregression(bmi ~ age + gender | id, data = Y, covariates = X)
print(model)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For more information, please refer to &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/kidzinski/kidzinski/fcomplete/fcomplete.pdf&#34; target=&#34;_blank&#34;&gt;the manual&lt;/a&gt; and to &lt;a href=&#34;https://github.com/kidzik/fcomplete/tree/master/vignettes&#34; target=&#34;_blank&#34;&gt;vignettes&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated human-level diagnosis of dysgraphia using a consumer tablet</title>
      <link>/publication/dysgraphia/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/dysgraphia/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>Estimating the effect size of surgery to improve walking in children with cerebral palsy from retrospective observational clinical data</title>
      <link>/publication/effect-size/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/effect-size/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>HealthAI working group</title>
      <link>/project/healthai/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/healthai/</guid>
      <description>&lt;p&gt;In April 2018 I started the &lt;a href=&#34;http://healthai.stanford.edu/&#34; target=&#34;_blank&#34;&gt;HealthAI working group&lt;/a&gt;. Our mission is to bring together experts from Schools of Medicine, Engineering, and Business at Stanford to tackle medical problems at scale using state-of-the-art AI methods. Three main&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Problems &amp;amp; solutions:&lt;/strong&gt; Find out what problems are researchers facing in medical fields. See new methods from CS researchers.
group&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hackathons:&lt;/strong&gt; Once we see some matches between problems and techniques we can approach them with quick &amp;lsquo;fail-fast&amp;rsquo; sessions.
settings&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lectures &amp;amp; workshops:&lt;/strong&gt; Learn new methods and tools applicable in your research from faculty, visiting professors and students.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://healthai.stanford.edu/&#34; target=&#34;_blank&#34;&gt;Find out more on the official website&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep learning in gait analysis</title>
      <link>/project/video-analysis/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/video-analysis/</guid>
      <description>&lt;p&gt;Stroke, Parkinson&amp;rsquo;s disease, cerebral palsy, multiple sclerosis, and other neurological and orthopedic conditions cause movement impairments that fundamentally limit patients&amp;rsquo; daily function and social participation. Quantitative assessment of movement is critical to medical decision making, but is currently possible only with expensive optical motion capture systems and highly-trained personnel, raising the cost of, and severely restricting access to this diagnostic data. The result of this constraint is limited and socio-economically unbalanced utilization of quantitative motion analyisis. These restrictions hinder early detection of disease, access to expert advice, large-scale research, and routine pre- and post-treatment monitoring. I proposed a method for estimating a variety of clinically relevant movement parameters from a video of a patient, allowing for quantitative gait analysis using commodity mobile devices.&lt;/p&gt;

&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/tqMDu6jYHLM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Principal component analysis of periodically correlated functional time series</title>
      <link>/publication/periodically-correlated/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/periodically-correlated/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>Orchestration Load Indicators and Patterns: In-the-wild Studies Using Mobile Eye-tracking</title>
      <link>/publication/orchestration/</link>
      <pubDate>Fri, 02 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/orchestration/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>How did we learn to walk?</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0800</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DeepArt - Neural style platform</title>
      <link>/project/deepart/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/deepart/</guid>
      <description>&lt;p&gt;Astonishing results from the &lt;a href=&#34;https://arxiv.org/pdf/1508.06576.pdf&#34; target=&#34;_blank&#34;&gt;original style transfer paper&lt;/a&gt; motivated me and Michał Warchoł to implement the first transfer platform and make it available to the public. After the prompt success we joined the forces with the authors of the orginal algorithm (Leon Gatys, Alexander Ecker and Mathias Bethge) and we continued working together on delivering tools for creating artworks within a few clicks. Visit &lt;a href=&#34;https://deepart.io/&#34; target=&#34;_blank&#34;&gt;deepart.io&lt;/a&gt; and try it out!&lt;/p&gt;

&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/olj6rktnr40&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FreeBoost - MRI segmentation</title>
      <link>/not-used/freeboost/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/not-used/freeboost/</guid>
      <description>&lt;p&gt;Together with my Stanford colleagues (Ben Kotopka, Alexander Onopa and Owen Philips) we developped an MRI brain segmentation tool. Find out more on the &lt;a href=&#34;http://freeboost.org/&#34; target=&#34;_blank&#34;&gt;project website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
