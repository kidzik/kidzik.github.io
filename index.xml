<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Łukasz Kidziński on Łukasz Kidziński</title>
    <link>/</link>
    <description>Recent content in Łukasz Kidziński on Łukasz Kidziński</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Łukasz Kidziński</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Recent &amp; Upcoming Talks</title>
      <link>/not-used/talks/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/not-used/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent Posts</title>
      <link>/not-used/posts/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/not-used/posts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Longitudinal data analysis using matrix completion</title>
      <link>/publication/fcomplete/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/fcomplete/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>fcomplete</title>
      <link>/project/longitudinal-data/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/longitudinal-data/</guid>
      <description>&lt;p&gt;Suppose we observe N subjects, each subject at multiple timepoints and we want to estimate a trajectory of progression of measurements in individual subjects. For example, suppose you observe BMI of N children at different ages, as presented below&lt;/p&gt;

&lt;p&gt;&lt;p align=&#34;center&#34;&gt;
   &lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/kidzinski/kidzinski/fcomplete/grouped.png&#34; width=450 /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Here, the connected dots come from individual subjects and the black thick line corresponds to the population mean.&lt;/p&gt;

&lt;p&gt;In this package we follow the methodology from &lt;a href=&#34;https://arxiv.org/abs/1809.08771&#34; target=&#34;_blank&#34;&gt;Kidziński, Hastie (2018)&lt;/a&gt; to fit trajectories using matrix completion. To this end, we discretize the time grid some continous basis and find a low-rank decomposition of the dense matrix.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/kidzinski/kidzinski/fcomplete/intro-1.png&#34; alt=&#34;Matrix completion and sparse longitudinal completion&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the classical matrix completion, we look for matrices &lt;code&gt;W&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt; that fit the observed points in &lt;code&gt;Y&lt;/code&gt; (green points in the image above). In our method, in order to impose smoothness, we additionaly assume the basis &lt;code&gt;B&lt;/code&gt; and again we look for the reprezentation minimizing the errror.&lt;/p&gt;

&lt;p&gt;The interface of the package is based on the mixed-effect models in &lt;code&gt;R&lt;/code&gt;. In particular, if we are given temporal observations &lt;code&gt;Y&lt;/code&gt; in the long format with columns &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;bmi&lt;/code&gt;, while additional covariates &lt;code&gt;X&lt;/code&gt;, constant over time are given as a data frame with columns &lt;code&gt;id&lt;/code&gt; and, say, &lt;code&gt;gender&lt;/code&gt;, we can fit the model by writing&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;model = fregression(bmi ~ age + gender | id, data = Y, covariates = X)
print(model)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For more information, please refer to &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/kidzinski/kidzinski/fcomplete/fcomplete.pdf&#34; target=&#34;_blank&#34;&gt;the manual&lt;/a&gt; and to &lt;a href=&#34;https://github.com/kidzik/fcomplete/tree/master/vignettes&#34; target=&#34;_blank&#34;&gt;vignettes&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated human-level diagnosis of dysgraphia using a consumer tablet</title>
      <link>/publication/dysgraphia/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/dysgraphia/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>Estimating the effect size of surgery to improve walking in children with cerebral palsy from retrospective observational clinical data</title>
      <link>/publication/effect-size/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/effect-size/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>HealthAI working group</title>
      <link>/project/healthai/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/healthai/</guid>
      <description>&lt;p&gt;Bringing together experts from Schools of Medicine, Engineering, and Business at Stanford to tackle medical problems at scale using state-of-the-art AI methods. Three main&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Problems &amp;amp; solutions:&lt;/strong&gt; Find out what problems are researchers facing in medical fields. See new methods from CS researchers.
group&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hackathons:&lt;/strong&gt; Once we see some matches between problems and techniques we can approach them with quick &amp;lsquo;fail-fast&amp;rsquo; sessions.
settings&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lectures &amp;amp; workshops:&lt;/strong&gt; Learn new methods and tools applicable in your research from faculty, visiting professors and students.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://healthai.stanford.edu/&#34; target=&#34;_blank&#34;&gt;Find out more on the official website&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep learning in gait analysis</title>
      <link>/project/video-analysis/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/video-analysis/</guid>
      <description>&lt;p&gt;Stroke, Parkinson&amp;rsquo;s disease, cerebral palsy, multiple sclerosis, and other neurological and orthopedic conditions cause movement impairments that fundamentally limit patients&amp;rsquo; daily function and social participation. Quantitative assessment of movement is critical to medical decision making, but is currently possible only with expensive optical motion capture systems and highly-trained personnel, raising the cost of, and severely restricting access to this diagnostic data. The result of this constraint is limited and socio-economically unbalanced utilization of quantitative motion analyisis. These restrictions hinder early detection of disease, access to expert advice, large-scale research, and routine pre- and post-treatment monitoring. We propose a method for estimating a variety of clinically relevant movement parameters from a video of a patient, allowing for quantitative gait analysis using commodity mobile devices.&lt;/p&gt;

&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/tqMDu6jYHLM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Principal component analysis of periodically correlated functional time series</title>
      <link>/publication/periodically-correlated/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/periodically-correlated/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>Orchestration Load Indicators and Patterns: In-the-wild Studies Using Mobile Eye-tracking</title>
      <link>/publication/orchestration/</link>
      <pubDate>Fri, 02 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/orchestration/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;
</description>
    </item>
    
    <item>
      <title>How did we learn to walk?</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0800</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DeepArt - Neural style platform</title>
      <link>/project/deepart/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/deepart/</guid>
      <description>&lt;p&gt;Astonishing results from the &lt;a href=&#34;https://arxiv.org/pdf/1508.06576.pdf&#34; target=&#34;_blank&#34;&gt;original style transfer paper&lt;/a&gt; motivated me and Michał Warchoł to implement the first transfer platform and make it available to the public. After the prompt success we joined the forces with the authors of the orginal algorithm (Leon Gatys, Alexander Ecker and Mathias Bethge) and we continued working together on delivering tools for creating artworks within a few clicks. Visit &lt;a href=&#34;https://deepart.io/&#34; target=&#34;_blank&#34;&gt;deepart.io&lt;/a&gt; and try it out!&lt;/p&gt;

&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/olj6rktnr40&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FreeBoost - MRI segmentation</title>
      <link>/not-used/freeboost/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/not-used/freeboost/</guid>
      <description>&lt;p&gt;Together with my Stanford colleagues (Ben Kotopka, Alexander Onopa and Owen Philips) we developped an MRI brain segmentation tool. Find out more on the &lt;a href=&#34;http://freeboost.org/&#34; target=&#34;_blank&#34;&gt;project website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Golf swing analysis app</title>
      <link>/not-used/kinemai/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/not-used/kinemai/</guid>
      <description>&lt;p&gt;During &lt;a href=&#34;https://www.gsb.stanford.edu/programs/stanford-ignite&#34; target=&#34;_blank&#34;&gt;Stanford Ignite progrem&lt;/a&gt;, together with my teammates we were evaluating commercial opportunities of applying AI for sport&amp;rsquo;s coaching. We prototyped a mobile app using pose estimation algorithms for improving golf swing performance.&lt;/p&gt;

&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/SjPP5x1csOc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning how to run (NIPS)</title>
      <link>/project/nips-learning-to-run/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/nips-learning-to-run/</guid>
      <description>

&lt;p&gt;Use my musculoskeletal reinforcement learning environment for other projects in computer science, neuroscience, biomechanics, etc.&lt;/p&gt;

&lt;p&gt;We set up a competition to build models of the brain in the &lt;a href=&#34;platform&#34; target=&#34;_blank&#34;&gt;crowdAI&lt;/a&gt;. Competition was accepted as one of the 5 official competitions at  &lt;a href=&#34;https://www.crowdai.org/challenges/nips-2017-learning-to-run&#34; target=&#34;_blank&#34;&gt;NIPS 2017&lt;/a&gt; and one of 8 competitions at &lt;a href=&#34;https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge&#34; target=&#34;_blank&#34;&gt;NIPS 2018&lt;/a&gt;, attracting over 500 teams from all over the world.&lt;/p&gt;

&lt;p&gt;Here are some solutions from the first challenge&lt;/p&gt;

&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/rhNxt0VccsE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;br&gt;
You can find the code on &lt;a href=&#34;http://osim-rl.stanford.edu/&#34; target=&#34;_blank&#34;&gt;the official website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Anaconda&lt;/strong&gt; is required to run our simulations. Anaconda will create a virtual environment with all the necessary libraries, to avoid conflicts with libraries in your operating system. You can get anaconda from here &lt;a href=&#34;https://www.continuum.io/downloads&#34; target=&#34;_blank&#34;&gt;https://www.continuum.io/downloads&lt;/a&gt;. In the following instructions we assume that Anaconda is successfully installed.&lt;/p&gt;

&lt;p&gt;For the challenge we prepared &lt;a href=&#34;http://opensim.stanford.edu/&#34; target=&#34;_blank&#34;&gt;OpenSim&lt;/a&gt; binaries as a conda environment to make the installation straightforward&lt;/p&gt;

&lt;p&gt;We support Windows, Linux, and Mac OSX (all in 64-bit). To install our simulator, you first need to create a conda environment with the OpenSim package.&lt;/p&gt;

&lt;p&gt;On &lt;strong&gt;Windows&lt;/strong&gt;, open a command prompt and type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda create -n opensim-rl -c kidzik opensim python=3.6.1
activate opensim-rl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On &lt;strong&gt;Linux/OSX&lt;/strong&gt;, run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda create -n opensim-rl -c kidzik opensim python=3.6.1
source activate opensim-rl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These commands will create a virtual environment on your computer with the necessary simulation libraries installed. Next, you need to install our python reinforcement learning environment. Type (on all platforms):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda install -c conda-forge lapack git
pip install git+https://github.com/stanfordnmbl/osim-rl.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the command &lt;code&gt;python -c &amp;quot;import opensim&amp;quot;&lt;/code&gt; runs smoothly, you are done! Otherwise, please refer to our &lt;a href=&#34;http://osim-rl.stanford.edu/docs/faq/&#34; target=&#34;_blank&#34;&gt;FAQ&lt;/a&gt; section.&lt;/p&gt;

&lt;p&gt;Note that &lt;code&gt;source activate opensim-rl&lt;/code&gt; activates the anaconda virtual environment. You need to type it every time you open a new terminal.&lt;/p&gt;

&lt;h2 id=&#34;basic-usage&#34;&gt;Basic usage&lt;/h2&gt;

&lt;p&gt;To execute 200 iterations of the simulation enter the &lt;code&gt;python&lt;/code&gt; interpreter and run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from osim.env import ProstheticsEnv

env = ProstheticsEnv(visualize=True)
observation = env.reset()
for i in range(200):
    observation, reward, done, info = env.step(env.action_space.sample())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/stanfordnmbl/osim-rl/1679344e509e29bdcc2ee368ddf83e868d93bf61/demo/random.gif&#34; alt=&#34;Random walk&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The function &lt;code&gt;env.action_space.sample()&lt;/code&gt; returns a random vector for muscle activations, so, in this example, muscles are activated randomly (red indicates an active muscle and blue an inactive muscle).  Clearly with this technique we won&amp;rsquo;t go too far.&lt;/p&gt;

&lt;p&gt;Your goal is to construct a controller, i.e. a function from the state space (current positions, velocities and accelerations of joints) to action space (muscle excitations), that will enable to model to travel as far as possible in a fixed amount of time. Suppose you trained a neural network mapping observations (the current state of the model) to actions (muscle excitations), i.e. you have a function &lt;code&gt;action = my_controller(observation)&lt;/code&gt;, then&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ...
total_reward = 0.0
for i in range(200):
    # make a step given by the controller and record the state and the reward
    observation, reward, done, info = env.step(my_controller(observation))
    total_reward += reward
    if done:
        break

# Your reward is
print(&amp;quot;Total reward %f&amp;quot; % total_reward)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can find details about the &lt;a href=&#34;http://osim-rl.stanford.edu/docs/nips2018/observation/&#34; target=&#34;_blank&#34;&gt;observation object here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
